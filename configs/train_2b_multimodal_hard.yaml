# DCLC 2B multimodal HARD: ultra-realistic image gen (incl. explicit). 4x T4 or single GPU.
# Vocab = text (50k) + visual codebook (16384) = 66384. Long training, image-heavy mix.

model:
  n_layers: 40
  hidden_dim: 2048
  n_heads: 16
  ffn_dim: 8192
  vocab_size: 66384   # 50000 text + 16384 visual (high fidelity)
  max_seq_len: 512
  codebook_size: 16384

training:
  batch_size: 1
  gradient_accumulation: 8    # 4 GPUs x 8 = 32 effective; 1 GPU = 8
  mixed_precision: true
  gradient_checkpointing: true
  max_steps: 100000          # train hard
  save_every: 1000
  lr: 4.0e-5
  warmup_steps: 2000
  max_grad_norm: 1.0
  data_dir: "data"
  text_only_prob: 0.2        # 80% image-caption steps for strong visual learning
  # LAION (additive): set laion_path to webdataset tars; laion_prob = fraction of image steps from LAION
  laion_path: null           # e.g. "data/laion/webdataset" or leave null
  laion_prob: 0.0            # e.g. 0.8 when using 2B LAION; 0 when not using LAION
  max_caption_len: 200
