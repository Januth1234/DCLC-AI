# DCLC 2B â€” Colab/Kaggle (16 GB GPU tight; 30 GB A100 recommended)
# ~2.1B params: 40 layers, 2048 dim, 16 heads, 8192 ffn

model:
  n_layers: 40
  hidden_dim: 2048
  n_heads: 16
  ffn_dim: 8192
  vocab_size: 50000
  max_seq_len: 512   # reduce if OOM on 16 GB (use 1024 on 30+ GB)

training:
  batch_size: 1
  gradient_accumulation: 32
  mixed_precision: true
  gradient_checkpointing: true
  max_steps: 12000
  save_every: 1500
  lr: 6.0e-5        # slightly lower for larger model
  warmup_steps: 500
  max_grad_norm: 1.0
  data_dir: "data"
